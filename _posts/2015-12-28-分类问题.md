---
layout: post
title: 分类问题
category: 人工智能
tags: [人工智能, 机器学习]
---

对于分类问题的输出,y不再是一个有连续值域的向量函数,而是逻辑函数.

<!--exerpt-->

## 假设函数

对于分类问题而言,我们通过叫做逻辑回归的方法给出假设.逻辑回归中用到的假设函数\\(h(x;\theta)\\)是S型函数:
\\[
h(x;\theta)=g((x,\theta))=\frac 1{1+e^{-(x,\theta)}}
\\]
S型函数将大致给出样本X为0或1的可能性.

## 决策边界

注意到有
\\[
\begin{aligned}
\lim_{x \to \infty}e^{-x} & = 0 \\\\\\
\lim_{x \to -\infty}e^{-x} & = \infty \\\\\\
e^0 & = 1
\end{aligned}
\\]
因此
\\(g(z)\geq0.5\\)当且仅当\\(z\geq0\\)
这时我们令
\\[
h(x)=
\begin{cases}
1 & h(x;\theta)\geq0.5 \\\\\\
0 & h(x;\theta)\lt0.5
\end{cases}
\\]

## 评估函数

对每一个样本,给出如下的评估:
\\[
cost(x,y) =
\begin{cases}
-\ln(h(x;\theta)) & y = 1 \\\\\\
-\ln(1-h(x;\theta)) & y = 0
\end{cases}
\\]
注意到:
\\[
cost(x,y) \to
\begin{cases}
0 & h(x;\theta) \to y \\\\\\
\infty & else
\end{cases}
\\]
将评估函数化简为:
\\[\overline{cost}(x,y) = -(y * \ln(h(x;\theta))+(1-y)*\ln(1-h(x;\theta)))\\]
定义:
\\[
\begin{aligned}
J(\theta) & = \frac 1m\sum_{i=1}^m\overline{cost}(x^{(i)},y^{(i)}) \\\\\\
& = -\frac 1m\sum_{i=1}^m(y^{(i)}\ln({h(x^{(i)};\theta}))+y^{(i)}\ln(1-h(x^{(i)};\theta)))
\end{aligned}
\\]

## 梯度下降

仍然使用梯度下降法进行优化.
\\[\theta := \theta - \alpha * \nabla J\\]
将\\(J(\theta)\\)代入得:
\\[
\begin{aligned}
\frac{\partial J}{\partial\theta} & = \frac{\rm dJ}{\rm dg} \frac{\partial g}{\partial\theta} \\\\\\
& = (\frac1{1+e^{-(x,\theta)}}-y)x \\\\\\
& = (h(x;\theta) - y)x
\end{aligned}
\\]
所以,
\\[\theta := \theta - \frac\alpha{m}X^T(g(\theta X)-y)\\]
除梯度下降法外,也有许多高级优化方法可以更快、更好地收敛

## 一对多策略

考虑到多类型分类问题时,简单地将\\(y\\)换作\\(y=(y_1,y_2,\cdots,y_n)^T\\),将一个多类问题转化为多个单类问题.