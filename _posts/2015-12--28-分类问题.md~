---
layout: post
title: 分类问题
category: 人工智能
tags: [人工智能, 机器学习]
---

对于分类问题的输出,\\(y\\)不再是一个有连续值域的向量函数,而是逻辑函数.

<!--exerpt-->

## 假设函数

对于分类问题而言,我们通过叫做逻辑回归的方法给出假设.逻辑回归中用到的假设函数$h_\theta(x)$是S型函数:
$$
h_\theta(x)=g(\theta^Tx)=\frac 1{1+e^{-\theta^Tx}}
$$
S型函数将大致给出样本X为0或1的可能性.

## 决策边界

注意到有
$$
\begin{aligned}
e^0 & = 1 \\\\
\lim_{x \to \infty}e^{-x} & = 0 \\\\
\lim_{x \to -\infty}e^{-x} & = \infty
\end{aligned}
$$
因此
$$g(z)\geq0.5当且仅当z\geq0$$
这时我们令
$$
h(x)=
\begin{cases}
1 & h_\theta(x)\geq0.5 \\\\
0 & h_\theta(x)\lt0.5
\end{cases}
$$

## 评估函数

对每一个样本,给出如下的评估:
$$cost(x,y) =
\begin{cases}
-ln(h_\theta(x)) & y = 1 \\\\
-ln(1-h_\theta(x)) & y = 0
\end{cases}
$$
注意到:
$$
cost(x,y) \to
\begin{cases}
0 & h_\theta(x) \to y \\\\
\infty & else
\end{cases}
$$
将评估函数化简为:
$$\overline{cost}(x,y) = -(y * ln(h_\theta(x))+(1-y)*ln(1-h_\theta(x)))$$
定义:
$$J(\theta) = \frac 1m\sum_{i=1}^m\overline{cost}(x^{(i)},y^{(i)})$$

## 梯度下降

仍然使用梯度下降法进行优化.
$$\theta := \theta - \alpha * \nabla J$$
将$J(\theta)$代入得:
$$\theta := \theta - \frac\alpha{m}X^T(g(\theta X)-y)$$
除梯度下降法外,也有许多高级优化方法可以更快、更好地收敛

## 一对多策略

考虑到多类型分类问题时,简单地将$y$换作$y=(y_1,y_2,\cdots,y_n)^T$,将一个多类问题转化为多个单类问题.